# Chapter 2: Using Pretrained Models and Tokenizers ğŸ› ï¸

## Overview ğŸŒŸ
Learn to use pretrained models and tokenizers from the Hugging Face Transformers library. This chapter covers loading models, tokenizing text, and running NLP tasks. ğŸš€

## Key Concepts ğŸ’¡
- **Pretrained Models** ğŸ¤–:
  - Models like BERT or GPT, ready for tasks like classification.
  - Access via `AutoModel` and `AutoTokenizer`.
- **Tokenization** âœ‚ï¸:
  - Converts text to numerical tokens (e.g., "Hello, world!" â†’ [101, 7592, 1010, 2088, 999, 102]).
- **Pipeline API** ğŸ®:
  - Simplifies tasks like text generation or classification.
- **Tokenizer API** ğŸ“:
  - Handles text preprocessing and batch processing.

## Practical Steps âœ…
1. **Load a Model and Tokenizer** âš™ï¸:
   ```python
   from transformers import AutoModelForSequenceClassification, AutoTokenizer
   model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
   tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
   ```
2. **Tokenize Text** ğŸ§‘â€ğŸ’»:
   ```python
   inputs = tokenizer("This is a test", return_tensors="pt")
   outputs = model(**inputs)
   print(outputs.logits)
   ```
3. **Batch Processing** ğŸ“š:
   ```python
   inputs = tokenizer(["Sentence 1", "Sentence 2"], return_tensors="pt", padding=True)
   outputs = model(**inputs)
   print(outputs.logits)
   ```
   See `code_examples/tokenizer_example.py` ğŸ.

## Tips for Beginners ğŸ’¡
- Try different models from the Hub ğŸŒ.
- Use `padding=True` for varying input lengths âœ….
- Check model cards for task details ğŸ“–.

## Resources ğŸ“š
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Code Example](../code_examples/tokenizer_example.py)
